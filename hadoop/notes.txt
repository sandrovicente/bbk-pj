Running MR tasks in Hadoop
--------------------------

*** Note: Only MR-1 and MR-2 have been covered.


START UP
------

1. Install Hadoop (2.3.0)

2. Set it up. This was used as reference set up test environment:
	http://www.drweiwang.com/install-hadoop-2-2-0-debian/

That includes configuration (conf/hadoop has the ones used for this project)
Creation of data and name nodes 
Formatting

	hdfs namenode -format

Startup

	$HADOOP_HOME/sbin/start-dfs.sh
	$HADOOP_HOME/sbin/start-yarn.sh

Check if these services are running:

	$ jps
	
	10001 Jps
	4322 NameNode
	4738 ResourceManager
	4835 NodeManager
	4565 SecondaryNameNode
	4412 DataNode

Run smoke test

hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.3.0.jar pi 2 5

---

EXECUTION

Go to bbk-pj/hadoop
Load the environment

	$ source ../bin/env.sh

Create folder in HDFS and copy log files into it (e.g. folder 'logs')

	$ hdfs dfs -mkdir logs
	$ hdfs dfs -copyFromLocal <local_logs> logs
	
MR1 - note that some paths are hardcoded. Haven't found a way to set 

	$ hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2.3.0.jar -mapper collector_map.sh -reducer "perl $COLMAT/collector_reduce.pl" -file collector_map.sh -file $COLMAT/collector_reduce.pl -file $COLMAT/collector.pl -file $COLMAT/ordererlib.pm -file $COLMAT/lm_env.pm -file $COLMAT/statlib.pm -input /user/sandro/logs/ -output /user/sandro/logs.1

MR2 - also note input should match output from MR1 (e.g. 'logs.1')

	$ hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2.3.0.jar -mapper "perl $COLMAT/merger_map.pl" -reducer "perl $COLMAT/merger_reduce.pl" -file $COLMAT/merger_reduce.pl -file $COLMAT/merger_map.pl -file $COLMAT/ordererlib.pm -file $COLMAT/lm_env.pm -file $COLMAT/statlib.pm -input /user/sandro/logs.1 -output /user/sandro/logs.1.1
	
Copy results to some local folder
	
	$ hdfs dfs -copyToLocal logs.1.1/part-00000 tmp/sip_hadoop.dmp

This should match the contents of 'sip_ordered.dmp' which is generated by the standalone process.
However, results are probably in different order.

	$ wc -l tmp/sip_hadoop.dmp
	3626 tmp/sip_hadoop.dmp
	
	$ wc -l tmp/sip_ordered.dmp
	3626 tmp/sip_ordered.dmp
